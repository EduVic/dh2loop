{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ranee/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/ranee/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ranee/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glove import Glove\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "import geopandas as gpd\n",
    "import os\n",
    "\n",
    "import json\n",
    "import h5py\n",
    "\n",
    "labelEncoder = LabelEncoder()\n",
    "one_enc = OneHotEncoder()\n",
    "lemma = nltk.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of original litho classes: 340\n",
      "number of litho classes : 78\n",
      "unclassified descriptions: 0\n"
     ]
    }
   ],
   "source": [
    "#Dir = '/mnt/d/Dropbox/Ranee_Joshi_PhD_Local/04_PythonCodes/dh2loop_old/shp_NSW'\n",
    "#DF=litho_Dataframe(Dir)\n",
    "#DF.to_csv('export.csv')\n",
    "DF = pd.read_csv('/mnt/d/Dropbox/Ranee_Joshi_PhD_Local/04_PythonCodes/dh2loop/notebooks/Upscaled_Litho_Test2.csv')\n",
    "DF['FromDepth'] = pd.to_numeric(DF.FromDepth)\n",
    "DF['ToDepth'] = pd.to_numeric(DF.ToDepth)\n",
    "DF['TopElev'] = pd.to_numeric(DF.TopElev)\n",
    "DF['BottomElev'] = pd.to_numeric(DF.BottomElev)\n",
    "DF['x'] = pd.to_numeric(DF.x)\n",
    "DF['y'] = pd.to_numeric(DF.y)\n",
    "print('number of original litho classes:', len(DF.MajorLithCode.unique()))\n",
    "print('number of litho classes :',\n",
    "          len(DF['reclass'].unique()))\n",
    "print('unclassified descriptions:',\n",
    "          len(DF[DF['reclass'].isnull()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_file(DF, name):\n",
    "    '''Function to save manually reclassified dataframe\n",
    "    Inputs:\n",
    "        -DF: reclassified pandas dataframe\n",
    "        -name: name (string) to save dataframe file\n",
    "    '''\n",
    "    DF.to_pickle('{}.pkl'.format(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_file(DF, 'manualTest_ygsb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_geovec(path):\n",
    "    instance = Glove()\n",
    "    with h5py.File(path, 'r') as f:\n",
    "        v = np.zeros(f['vectors'].shape, f['vectors'].dtype)\n",
    "        f['vectors'].read_direct(v)\n",
    "        dct = f['dct'][()].tostring().decode('utf-8')\n",
    "        dct = json.loads(dct)\n",
    "    instance.word_vectors = v\n",
    "    instance.no_components = v.shape[1]\n",
    "    instance.word_biases = np.zeros(v.shape[0])\n",
    "    instance.add_dictionary(dct)\n",
    "    return instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords\n",
    "extra_stopwords = [\n",
    "    'also',\n",
    "]\n",
    "stop = stopwords.words('english') + extra_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text, min_len=1):\n",
    "    '''Function that tokenize a set of strings\n",
    "    Input:\n",
    "        -text: set of strings\n",
    "        -min_len: tokens length\n",
    "    Output:\n",
    "        -list containing set of tokens'''\n",
    "\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text)\n",
    "              for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "\n",
    "    for token in tokens:\n",
    "        if token.isalpha() and len(token) >= min_len:\n",
    "            filtered_tokens.append(token)\n",
    "\n",
    "    return [x.lower() for x in filtered_tokens if x not in stop]\n",
    "\n",
    "\n",
    "def tokenize_and_lemma(text, min_len=0):\n",
    "    '''Function that retrieves lemmatised tokens\n",
    "    Inputs:\n",
    "        -text: set of strings\n",
    "        -min_len: length of text\n",
    "    Outputs:\n",
    "        -list containing lemmatised tokens'''\n",
    "    filtered_tokens = tokenize(text, min_len=min_len)\n",
    "\n",
    "    lemmas = [lemma.lemmatize(t) for t in filtered_tokens]\n",
    "    return lemmas\n",
    "\n",
    "\n",
    "def get_vector(word, model, return_zero=False):\n",
    "    '''Function that retrieves word embeddings (vector)\n",
    "    Inputs:\n",
    "        -word: token (string)\n",
    "        -model: trained MLP model\n",
    "        -return_zero: boolean variable\n",
    "    Outputs:\n",
    "        -wv: numpy array (vector)'''\n",
    "    epsilon = 1.e-10\n",
    "\n",
    "    unk_idx = model.dictionary['unk']\n",
    "    idx = model.dictionary.get(word, unk_idx)\n",
    "    wv = model.word_vectors[idx].copy()\n",
    "\n",
    "    if return_zero and word not in model.dictionary:\n",
    "        n_comp = model.word_vectors.shape[1]\n",
    "        wv = np.zeros(n_comp) + epsilon\n",
    "\n",
    "    return wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_embeddings(dataframe_file, model):\n",
    "    '''Function to retrieve sentence embeddings from dataframe with\n",
    "    lithological descriptions.\n",
    "    Inputs:\n",
    "        -dataframe_file: pandas dataframe containing lithological descriptions\n",
    "                         and reclassified lithologies\n",
    "        -model: word embeddings model generated using GloVe\n",
    "    Outputs:\n",
    "        -DF: pandas dataframe including sentence embeddings'''\n",
    "    DF = pd.read_pickle(dataframe_file)\n",
    "    DF = DF.drop_duplicates(subset=['x', 'y', 'z'])\n",
    "    DF['tokens'] = DF['Description'].apply(lambda x: tokenize_and_lemma(x))\n",
    "    DF['length'] = DF['tokens'].apply(lambda x: len(x))\n",
    "    DF = DF.loc[DF['length']> 0]\n",
    "    DF['vectors'] = DF['tokens'].apply(lambda x: np.asarray([get_vector(n, model) for n in x]))\n",
    "    DF['mean'] = DF['vectors'].apply(lambda x: np.mean(x[~np.all(x == 1.e-10, axis=1)], axis=0))\n",
    "    DF['reclass'] = pd.Categorical(DF.reclass)\n",
    "    DF['code'] = DF.reclass.cat.codes\n",
    "    DF['drop'] = DF['mean'].apply(lambda x: (~np.isnan(x).any()))\n",
    "    DF = DF[DF['drop']]\n",
    "    return DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading word embeddings model\n",
    "# (This can be obtained from https://github.com/spadarian/GeoVec )\n",
    "#modelEmb = Glove.load('/home/ignacio/Documents/chapter2/best_glove_300_317413_w10_lemma.pkl')\n",
    "modelEmb = load_geovec('geovec_300d_v1.h5')\n",
    "\n",
    "# getting the mean embeddings of descriptions\n",
    "DF = mean_embeddings('manualTest_ygsb.pkl', modelEmb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      Description  HydroCode  FromDepth  ToDepth     TopElev  \\\n",
      "0            Transported  hardpan     124897          0      1.0  400.000000   \n",
      "1            Transported  hardpan     124897          1      2.0  399.133975   \n",
      "2                  Regolith  Clay     124897          2      3.0  398.267949   \n",
      "3                  Regolith  Clay     124897          3      4.0  397.401924   \n",
      "4                  Regolith  Clay     124897          4      5.0  396.535898   \n",
      "...                           ...        ...        ...      ...         ...   \n",
      "159942                     Basalt    4167632         25     28.0  375.000000   \n",
      "159943                     Basalt    4167632         28     29.0  372.000000   \n",
      "159944  Banded Iron Formation BIF    4167633          1      4.0  399.000000   \n",
      "159945                     Basalt    4167633          4      5.0  396.000000   \n",
      "159946  Banded Iron Formation BIF    4167634         46     47.0  354.000000   \n",
      "\n",
      "        BottomElev MajorLithCode  geometry           z  \\\n",
      "0       399.133975           NHO       NaN  399.566987   \n",
      "1       398.267949           NHO       NaN  398.700962   \n",
      "2       397.401924           RCY       NaN  397.834937   \n",
      "3       396.535898           RCY       NaN  396.968911   \n",
      "4       395.669873           RCY       NaN  396.102886   \n",
      "...            ...           ...       ...         ...   \n",
      "159942  372.000000           MBA       NaN  373.500000   \n",
      "159943  371.000000           MBA       NaN  371.500000   \n",
      "159944  396.000000           SBI       NaN  397.500000   \n",
      "159945  395.000000           MBA       NaN  395.500000   \n",
      "159946  353.000000           SBI       NaN  353.500000   \n",
      "\n",
      "                              OWN    reclass            x            y  \\\n",
      "0            Transported  hardpan  colluvium  513822.0449  6952747.766   \n",
      "1            Transported  hardpan  colluvium  513822.5449  6952747.766   \n",
      "2                  Regolith  Clay        mud  513823.0449  6952747.766   \n",
      "3                  Regolith  Clay        mud  513823.5449  6952747.766   \n",
      "4                  Regolith  Clay        mud  513824.0449  6952747.766   \n",
      "...                           ...        ...          ...          ...   \n",
      "159942                     Basalt     basalt  576956.0811  6882354.387   \n",
      "159943                     Basalt     basalt  576956.0811  6882354.387   \n",
      "159944  Banded Iron Formation BIF      Other  577054.2428  6882353.752   \n",
      "159945                     Basalt     basalt  577054.2428  6882353.752   \n",
      "159946  Banded Iron Formation BIF      Other  577152.4045  6882353.117   \n",
      "\n",
      "                                tokens  length  \\\n",
      "0               [transported, hardpan]       2   \n",
      "1               [transported, hardpan]       2   \n",
      "2                     [regolith, clay]       2   \n",
      "3                     [regolith, clay]       2   \n",
      "4                     [regolith, clay]       2   \n",
      "...                                ...     ...   \n",
      "159942                        [basalt]       1   \n",
      "159943                        [basalt]       1   \n",
      "159944  [banded, iron, formation, bif]       4   \n",
      "159945                        [basalt]       1   \n",
      "159946  [banded, iron, formation, bif]       4   \n",
      "\n",
      "                                                  vectors  \\\n",
      "0       [[-0.18778053, -0.0063699493, -0.016271375, -0...   \n",
      "1       [[-0.18778053, -0.0063699493, -0.016271375, -0...   \n",
      "2       [[0.029590942, 0.08967461, -0.16049796, -0.342...   \n",
      "3       [[0.029590942, 0.08967461, -0.16049796, -0.342...   \n",
      "4       [[0.029590942, 0.08967461, -0.16049796, -0.342...   \n",
      "...                                                   ...   \n",
      "159942  [[-0.2746381, -0.23468062, -0.37358257, 0.0047...   \n",
      "159943  [[-0.2746381, -0.23468062, -0.37358257, 0.0047...   \n",
      "159944  [[0.02457981, -0.39863607, 0.04441907, 0.09330...   \n",
      "159945  [[-0.2746381, -0.23468062, -0.37358257, 0.0047...   \n",
      "159946  [[0.02457981, -0.39863607, 0.04441907, 0.09330...   \n",
      "\n",
      "                                                     mean  code  drop  \n",
      "0       [-0.10188103, -0.009773923, -0.030616358, -0.1...    14  True  \n",
      "1       [-0.10188103, -0.009773923, -0.030616358, -0.1...    14  True  \n",
      "2       [0.09962925, -0.10881818, -0.29363656, -0.2486...    44  True  \n",
      "3       [0.09962925, -0.10881818, -0.29363656, -0.2486...    44  True  \n",
      "4       [0.09962925, -0.10881818, -0.29363656, -0.2486...    44  True  \n",
      "...                                                   ...   ...   ...  \n",
      "159942  [-0.2746381, -0.23468062, -0.37358257, 0.00474...     4  True  \n",
      "159943  [-0.2746381, -0.23468062, -0.37358257, 0.00474...     4  True  \n",
      "159944  [0.010618463, -0.33650404, -0.120667905, -0.02...     0  True  \n",
      "159945  [-0.2746381, -0.23468062, -0.37358257, 0.00474...     4  True  \n",
      "159946  [0.010618463, -0.33650404, -0.120667905, -0.02...     0  True  \n",
      "\n",
      "[122502 rows x 19 columns]\n"
     ]
    }
   ],
   "source": [
    "DF2 = DF[DF['code'].isin(DF['code'].value_counts()[DF['code'].value_counts()>2].index)]\n",
    "print(DF2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_stratified_dataset(Dataframe, test_size, validation_size):\n",
    "    '''Function that split dataset into test, training and validation subsets\n",
    "    Inputs:\n",
    "        -Dataframe: pandas dataframe with sentence mean_embeddings\n",
    "        -test_size: decimal number to generate the test subset\n",
    "        -validation_size: decimal number to generate the validation subset\n",
    "    Outputs:\n",
    "        -X: numpy array with embeddings\n",
    "        -Y: numpy array with lithological classes\n",
    "        -X_test: numpy array with embeddings for test subset\n",
    "        -Y_test: numpy array with lithological classes for test subset\n",
    "        -Xt: numpy array with embeddings for training subset\n",
    "        -yt: numpy array with lithological classes for training subset\n",
    "        -Xv: numpy array with embeddings for validation subset\n",
    "        -yv: numpy array with lithological classes for validation subset\n",
    "        '''\n",
    "    #df2 = Dataframe[Dataframe['code'].isin(Dataframe['code'].value_counts()[Dataframe['code'].value_counts()>2].index)]\n",
    "    #X = np.vstack(df2['mean'].values)\n",
    "    #Y = df2.code.values.reshape(len(df2.code), 1)\n",
    "    X = np.vstack(Dataframe['mean'].values)\n",
    "    Y = Dataframe.code.values.reshape(len(Dataframe.code), 1)\n",
    "    #print(X.shape)\n",
    "    #print (Dataframe.code.values.shape)\n",
    "    #print (len(Dataframe.code))\n",
    "    #print (Y.shape)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                        Y, stratify=Y,\n",
    "                                                        test_size=test_size,\n",
    "                                                        random_state=42)\n",
    "    #print(X_train.shape)\n",
    "    #print(Y_train.shape)\n",
    "    Xt, Xv, yt, yv = train_test_split(X_train,\n",
    "                                      y_train,\n",
    "                                      test_size=validation_size,\n",
    "                                      stratify=None,\n",
    "                                      random_state=1)\n",
    "    return X, Y, X_test, y_test, Xt, yt, Xv, yv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      " - 8s - loss: 0.2053 - accuracy: 0.9599\n",
      "Epoch 2/30\n",
      " - 7s - loss: 0.0070 - accuracy: 0.9988\n",
      "Epoch 3/30\n",
      " - 6s - loss: 0.0036 - accuracy: 0.9994\n",
      "Epoch 4/30\n",
      " - 6s - loss: 0.0034 - accuracy: 0.9995\n",
      "Epoch 5/30\n",
      " - 6s - loss: 0.0033 - accuracy: 0.9993\n",
      "Epoch 6/30\n",
      " - 6s - loss: 0.0024 - accuracy: 0.9996\n",
      "Epoch 7/30\n",
      " - 6s - loss: 0.0020 - accuracy: 0.9997\n",
      "Epoch 8/30\n",
      " - 7s - loss: 0.0016 - accuracy: 0.9998\n",
      "Epoch 9/30\n",
      " - 7s - loss: 0.0042 - accuracy: 0.9992\n",
      "Epoch 10/30\n",
      " - 6s - loss: 0.0023 - accuracy: 0.9996\n",
      "Epoch 11/30\n",
      " - 6s - loss: 0.0017 - accuracy: 0.9997\n",
      "Epoch 12/30\n",
      " - 6s - loss: 0.0016 - accuracy: 0.9998\n",
      "Epoch 13/30\n",
      " - 6s - loss: 0.0033 - accuracy: 0.9993\n",
      "Epoch 14/30\n",
      " - 6s - loss: 0.0019 - accuracy: 0.9996\n",
      "Epoch 15/30\n",
      " - 6s - loss: 0.0015 - accuracy: 0.9998\n",
      "Epoch 16/30\n",
      " - 7s - loss: 0.0015 - accuracy: 0.9998\n",
      "Epoch 17/30\n",
      " - 7s - loss: 0.0016 - accuracy: 0.9998\n",
      "Epoch 18/30\n",
      " - 7s - loss: 0.0016 - accuracy: 0.9998\n",
      "Epoch 19/30\n",
      " - 6s - loss: 0.0016 - accuracy: 0.9998\n",
      "Epoch 20/30\n",
      " - 8s - loss: 0.0038 - accuracy: 0.9994\n",
      "Epoch 21/30\n",
      " - 7s - loss: 0.0023 - accuracy: 0.9995\n",
      "Epoch 22/30\n",
      " - 6s - loss: 0.0015 - accuracy: 0.9998\n",
      "Epoch 23/30\n",
      " - 6s - loss: 0.0015 - accuracy: 0.9998\n",
      "Epoch 24/30\n",
      " - 6s - loss: 0.0015 - accuracy: 0.9998\n",
      "Epoch 25/30\n",
      " - 6s - loss: 0.0015 - accuracy: 0.9998\n",
      "Epoch 26/30\n",
      " - 6s - loss: 0.0045 - accuracy: 0.9993\n",
      "Epoch 27/30\n",
      " - 6s - loss: 0.0022 - accuracy: 0.9996\n",
      "Epoch 28/30\n",
      " - 6s - loss: 0.0015 - accuracy: 0.9997\n",
      "Epoch 29/30\n",
      " - 6s - loss: 0.0019 - accuracy: 0.9996\n",
      "Epoch 30/30\n",
      " - 6s - loss: 0.0024 - accuracy: 0.9996\n"
     ]
    }
   ],
   "source": [
    "# subseting dataset for training classifier\n",
    "X, Y, X_test, Y_test, X_train, Y_train, X_validation, Y_validation = split_stratified_dataset(DF2, 0.1, 0.1)\n",
    "\n",
    "# encoding lithological classes\n",
    "encodes = one_enc.fit_transform(Y_train).toarray()\n",
    "\n",
    "# MLP model generation\n",
    "model = Sequential()\n",
    "model.add(Dense(100, input_dim=300, activation='relu'))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(units=len(DF2.code.unique()), activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# training MLP model\n",
    "model.fit(X_train, encodes, epochs=30, batch_size=100, verbose=2)\n",
    "\n",
    "# saving MLP model\n",
    "model.save('mlp_prob_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_predictions(classifier, x):\n",
    "    '''Function that retrieves lithological classes using the trained classifier\n",
    "    Inputs:\n",
    "        -classifier: trained MLP classifier\n",
    "        -x: numpy array containing embbedings\n",
    "    Outputs:\n",
    "        -codes_pred: numpy array containing lithological classes predicted'''\n",
    "    preds = classifier.predict(x, verbose=0)\n",
    "    new_onehot = np.zeros((x.shape[0], 72))\n",
    "    new_onehot[np.arange(len(preds)), preds.argmax(axis=1)] = 1\n",
    "    codes_pred = one_enc.inverse_transform(new_onehot)\n",
    "    return codes_pred\n",
    "\n",
    "\n",
    "def classifier_assess(classifier, x, y):\n",
    "    '''Function that prints the performance of the classifier\n",
    "    Inputs:\n",
    "        -classifier: trained MLP classifier\n",
    "        -x: numpy array with embeddings\n",
    "        -y: numpy array with lithological classes predicted'''\n",
    "    Y2 = retrieve_predictions(classifier, x)\n",
    "    print('f1 score: ', metrics.f1_score(y, Y2, average='macro'),\n",
    "          'accuracy: ', metrics.accuracy_score(y, Y2),\n",
    "          'balanced_accuracy:', metrics.balanced_accuracy_score(y, Y2))\n",
    "\n",
    "\n",
    "def save_predictions(Dataframe, classifier, x, name):\n",
    "    '''Function that saves dataframe predictions as a pickle file\n",
    "    Inputs:\n",
    "        -Dataframe: pandas dataframe with mean_embeddings\n",
    "        -classifier: trained MLP model,\n",
    "        -x: numpy array with embeddings,\n",
    "        -name: string name to save dataframe\n",
    "    Outputs:\n",
    "        -save dataframe'''\n",
    "    preds = classifier.predict(x, verbose=0)\n",
    "    Dataframe['predicted_probabilities'] = preds.tolist()\n",
    "    Dataframe['pred'] = retrieve_predictions(classifier, x).astype(np.int32)\n",
    "    Dataframe[['x', 'y', 'FromDepth', 'ToDepth', 'TopElev', 'BottomElev',\n",
    "               'mean', 'predicted_probabilities', 'pred', 'reclass', 'code']].to_pickle('{}.pkl'.format(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score:  1.0 accuracy:  1.0 balanced_accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# assessment of model performance\n",
    "classifier_assess(model, X_validation, Y_validation)\n",
    "\n",
    "# save lithological prediction likelihoods dataframe\n",
    "save_predictions(DF2, model, X, 'YGSBpredictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('YGSBpredictions.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  x            y  FromDepth  ToDepth     TopElev  BottomElev  \\\n",
      "0       513822.0449  6952747.766          0      1.0  400.000000  399.133975   \n",
      "1       513822.5449  6952747.766          1      2.0  399.133975  398.267949   \n",
      "2       513823.0449  6952747.766          2      3.0  398.267949  397.401924   \n",
      "3       513823.5449  6952747.766          3      4.0  397.401924  396.535898   \n",
      "4       513824.0449  6952747.766          4      5.0  396.535898  395.669873   \n",
      "...             ...          ...        ...      ...         ...         ...   \n",
      "159942  576956.0811  6882354.387         25     28.0  375.000000  372.000000   \n",
      "159943  576956.0811  6882354.387         28     29.0  372.000000  371.000000   \n",
      "159944  577054.2428  6882353.752          1      4.0  399.000000  396.000000   \n",
      "159945  577054.2428  6882353.752          4      5.0  396.000000  395.000000   \n",
      "159946  577152.4045  6882353.117         46     47.0  354.000000  353.000000   \n",
      "\n",
      "                                                     mean  \\\n",
      "0       [-0.10188103, -0.009773923, -0.030616358, -0.1...   \n",
      "1       [-0.10188103, -0.009773923, -0.030616358, -0.1...   \n",
      "2       [0.09962925, -0.10881818, -0.29363656, -0.2486...   \n",
      "3       [0.09962925, -0.10881818, -0.29363656, -0.2486...   \n",
      "4       [0.09962925, -0.10881818, -0.29363656, -0.2486...   \n",
      "...                                                   ...   \n",
      "159942  [-0.2746381, -0.23468062, -0.37358257, 0.00474...   \n",
      "159943  [-0.2746381, -0.23468062, -0.37358257, 0.00474...   \n",
      "159944  [0.010618463, -0.33650404, -0.120667905, -0.02...   \n",
      "159945  [-0.2746381, -0.23468062, -0.37358257, 0.00474...   \n",
      "159946  [0.010618463, -0.33650404, -0.120667905, -0.02...   \n",
      "\n",
      "                                  predicted_probabilities  pred    reclass  \\\n",
      "0       [1.2841216978642933e-10, 1.470962351923788e-09...    14  colluvium   \n",
      "1       [1.2841216978642933e-10, 1.470962351923788e-09...    14  colluvium   \n",
      "2       [9.695031105406576e-24, 0.0, 0.0, 1.1913997890...    44        mud   \n",
      "3       [9.695031105406576e-24, 0.0, 0.0, 1.1913997890...    44        mud   \n",
      "4       [9.695031105406576e-24, 0.0, 0.0, 1.1913997890...    44        mud   \n",
      "...                                                   ...   ...        ...   \n",
      "159942  [0.0, 0.0, 0.0, 0.0, 1.0, 2.4139207354177097e-...     4     basalt   \n",
      "159943  [0.0, 0.0, 0.0, 0.0, 1.0, 2.4139207354177097e-...     4     basalt   \n",
      "159944  [0.9999998807907104, 1.2419753713496011e-19, 1...     0      Other   \n",
      "159945  [0.0, 0.0, 0.0, 0.0, 1.0, 2.4139207354177097e-...     4     basalt   \n",
      "159946  [0.9999998807907104, 1.2419753713496011e-19, 1...     0      Other   \n",
      "\n",
      "        code  \n",
      "0         14  \n",
      "1         14  \n",
      "2         44  \n",
      "3         44  \n",
      "4         44  \n",
      "...      ...  \n",
      "159942     4  \n",
      "159943     4  \n",
      "159944     0  \n",
      "159945     4  \n",
      "159946     0  \n",
      "\n",
      "[122502 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122502"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>FromDepth</th>\n",
       "      <th>ToDepth</th>\n",
       "      <th>TopElev</th>\n",
       "      <th>BottomElev</th>\n",
       "      <th>mean</th>\n",
       "      <th>predicted_probabilities</th>\n",
       "      <th>pred</th>\n",
       "      <th>reclass</th>\n",
       "      <th>code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>513822.0449</td>\n",
       "      <td>6952747.766</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>399.133975</td>\n",
       "      <td>[-0.10188103, -0.009773923, -0.030616358, -0.1...</td>\n",
       "      <td>[1.2841216978642933e-10, 1.470962351923788e-09...</td>\n",
       "      <td>14</td>\n",
       "      <td>colluvium</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>513822.5449</td>\n",
       "      <td>6952747.766</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>399.133975</td>\n",
       "      <td>398.267949</td>\n",
       "      <td>[-0.10188103, -0.009773923, -0.030616358, -0.1...</td>\n",
       "      <td>[1.2841216978642933e-10, 1.470962351923788e-09...</td>\n",
       "      <td>14</td>\n",
       "      <td>colluvium</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>513823.0449</td>\n",
       "      <td>6952747.766</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>398.267949</td>\n",
       "      <td>397.401924</td>\n",
       "      <td>[0.09962925, -0.10881818, -0.29363656, -0.2486...</td>\n",
       "      <td>[9.695031105406576e-24, 0.0, 0.0, 1.1913997890...</td>\n",
       "      <td>44</td>\n",
       "      <td>mud</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>513823.5449</td>\n",
       "      <td>6952747.766</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>397.401924</td>\n",
       "      <td>396.535898</td>\n",
       "      <td>[0.09962925, -0.10881818, -0.29363656, -0.2486...</td>\n",
       "      <td>[9.695031105406576e-24, 0.0, 0.0, 1.1913997890...</td>\n",
       "      <td>44</td>\n",
       "      <td>mud</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>513824.0449</td>\n",
       "      <td>6952747.766</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>396.535898</td>\n",
       "      <td>395.669873</td>\n",
       "      <td>[0.09962925, -0.10881818, -0.29363656, -0.2486...</td>\n",
       "      <td>[9.695031105406576e-24, 0.0, 0.0, 1.1913997890...</td>\n",
       "      <td>44</td>\n",
       "      <td>mud</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             x            y  FromDepth  ToDepth     TopElev  BottomElev  \\\n",
       "0  513822.0449  6952747.766          0      1.0  400.000000  399.133975   \n",
       "1  513822.5449  6952747.766          1      2.0  399.133975  398.267949   \n",
       "2  513823.0449  6952747.766          2      3.0  398.267949  397.401924   \n",
       "3  513823.5449  6952747.766          3      4.0  397.401924  396.535898   \n",
       "4  513824.0449  6952747.766          4      5.0  396.535898  395.669873   \n",
       "\n",
       "                                                mean  \\\n",
       "0  [-0.10188103, -0.009773923, -0.030616358, -0.1...   \n",
       "1  [-0.10188103, -0.009773923, -0.030616358, -0.1...   \n",
       "2  [0.09962925, -0.10881818, -0.29363656, -0.2486...   \n",
       "3  [0.09962925, -0.10881818, -0.29363656, -0.2486...   \n",
       "4  [0.09962925, -0.10881818, -0.29363656, -0.2486...   \n",
       "\n",
       "                             predicted_probabilities  pred    reclass  code  \n",
       "0  [1.2841216978642933e-10, 1.470962351923788e-09...    14  colluvium    14  \n",
       "1  [1.2841216978642933e-10, 1.470962351923788e-09...    14  colluvium    14  \n",
       "2  [9.695031105406576e-24, 0.0, 0.0, 1.1913997890...    44        mud    44  \n",
       "3  [9.695031105406576e-24, 0.0, 0.0, 1.1913997890...    44        mud    44  \n",
       "4  [9.695031105406576e-24, 0.0, 0.0, 1.1913997890...    44        mud    44  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = data['predicted_probabilities'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.to_csv('YGSBpredictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.8.4'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import striplog\n",
    "striplog.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from striplog import Lexicon, Component, Position, Interval, Decor, Legend, Striplog\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "legend = Legend.builtin('NSDOE')\n",
    "lexicon = Lexicon.default()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = Striplog.from_csv(text=data, stop=650)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
